{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArchanaAhlawat7/llm_experiments/blob/main/Mistral7b_finetune_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1Elqqe4Kx4IH"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install peft\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes\n",
        "# !pip install evaluate # if calculating bleu and rouge\n",
        "# !pip install rouge_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data set-up"
      ],
      "metadata": {
        "id": "H-090jrhIkPf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfuxNhXJzQDw",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dolly_ds = load_dataset(\"databricks/databricks-dolly-15k\")\n",
        "dolly_df = pd.DataFrame(dolly_ds['train'].select(range(10000)))\n",
        "\n",
        "# Append instructions + context (if exists) + response.\n",
        "def create_input(row):\n",
        "    if pd.notna(row['context']) and row['context'].strip():\n",
        "        return f\"Instruction: {row['instruction']} \\n\\nContext: {row['context']} \\n\\nResponse: {row['response']}\"\n",
        "    else:\n",
        "        return f\"Instruction: {row['instruction']} \\n\\nResponse: {row['response']}\"\n",
        "\n",
        "dolly_df['input'] = dolly_df.apply(create_input, axis=1)\n",
        "\n",
        "train_df, test_df = train_test_split(dolly_df, train_size=0.8, test_size=0.2, random_state=42)\n",
        "train_df, val_df = train_test_split(train_df, train_size=0.8, test_size=0.2, random_state=42)\n",
        "\n",
        "train_df.drop(columns=['instruction', 'context', 'response', 'category'], inplace=True)\n",
        "val_df.drop(columns=['instruction', 'context', 'response', 'category'], inplace=True)\n",
        "test_df.drop(columns=['input','category'], inplace=True)\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_df)\n",
        "test_ds = Dataset.from_pandas(test_df)\n",
        "val_ds = Dataset.from_pandas(val_df)\n",
        "\n",
        "# Check on structure of datasets\n",
        "print(train_ds.shape)\n",
        "print(test_ds.shape)\n",
        "print(val_ds.shape)\n",
        "\n",
        "print(train_ds[:2])\n",
        "print(test_ds[:2])\n",
        "print(val_ds[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "nG1_SrroIpMf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ftYTIYF7zlVK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "# select one for finetuning\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, load_in_8bit=True, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, trust_remote_code=True)\n",
        "\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=64,\n",
        "    target_modules= [\"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"lm_head\"\n",
        "      ],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Mistral Finetune w just attention layers: trainable params: 6,815,744 || all params: 7,248,547,840 || trainable%: 0.0940\n",
        "# Mistral Finetune w all linear layers: trainable params: 42,520,576 || all params: 7,284,252,672 || trainable%: 0.5837\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvJ8Fye712ve",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Set up tokenizer and tokenize train and val sets, to be used in training.\n",
        "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained( # eos and bos during training, but not during inference.\n",
        "    base_model_id,\n",
        "    padding_side=\"left\",\n",
        "    add_eos_token=True,\n",
        "    add_bos_token=True,\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_input(prompt):\n",
        "  result = tokenizer(\n",
        "      prompt['input'],\n",
        "      truncation=True,\n",
        "      max_length=512,\n",
        "      padding=\"max_length\",\n",
        "  )\n",
        "  result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "  return result\n",
        "\n",
        "tokenized_train = train_ds.map(tokenize_input)\n",
        "tokenized_val = val_ds.map(tokenize_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDfQLUuN0hCG"
      },
      "outputs": [],
      "source": [
        "# !pip install trl\n",
        "import transformers\n",
        "\n",
        "# Set up training configs\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    args=transformers.TrainingArguments(\n",
        "        output_dir='./mistral_four',\n",
        "        warmup_steps=1,\n",
        "        per_device_train_batch_size=8,\n",
        "        gradient_accumulation_steps=4,\n",
        "        gradient_checkpointing=True,\n",
        "        num_train_epochs=4,\n",
        "        learning_rate=1e-4,\n",
        "        fp16=True,\n",
        "        fp16_full_eval=True,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        logging_steps=25,\n",
        "        logging_dir=\"./logs_debugging\",\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=50,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=25,\n",
        "        do_eval=True,\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing the base model with finetuned models."
      ],
      "metadata": {
        "id": "pp5sq8G3c271"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig, AutoPeftModelForCausalLM\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# load the model\n",
        "peft_model_id = \"Archanaa7/ft_mistral_dolly\" # finetune #1\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "\n",
        "ft_one = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    device_map={\"\":0},\n",
        "    trust_remote_code=True,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "ft_one = PeftModel.from_pretrained(ft_one, peft_model_id)\n",
        "ft_one.to(\"cuda\")\n",
        "\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-v0.1\",\n",
        "    load_in_4bit=True,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Alternative way of loading PEFT model\n",
        "ft_overfit = AutoPeftModelForCausalLM.from_pretrained(pretrained_model_name_or_path=\"Archanaa7/ft_mistral_dolly_secondtry\", load_in_4bit=True) # finetune #2 (overfit)\n",
        "\n",
        "ft_overfit.get_model_status()\n",
        "ft_overfit.to(\"cuda\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hhEcih_3FLHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Qualitative tests. Sanity check.\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-v0.1\",\n",
        "    padding_side=\"left\",\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "input = tokenizer(\"Instruction: What type of effect does Coffee have on humans? \\n\\nContext: 'Coffee is a beverage prepared from roasted coffee beans. Darkly colored, bitter, and slightly acidic, coffee has a stimulating effect on humans, primarily due to its caffeine content. It has the highest sales in the world market for hot drinks.\\n\\nSeeds of the Coffee plant\\'s fruits are separated to produce un-roasted green coffee beans. The beans are roasted and then ground into fine particles that are typically steeped in hot water before being filtered out, producing a cup of coffee. It is usually served hot, although chilled or iced coffee is common. Coffee can be prepared and presented in a variety of ways (e.g., espresso, French press, caff√® latte, or already-brewed canned coffee). Sugar, sugar substitutes, milk, and cream are often used to mask the bitter taste or enhance the flavor.'\\n\\n Response: \", return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "base.eval()\n",
        "with torch.no_grad():\n",
        "  gen_output = base.generate(**input, max_new_tokens=100, repetition_penalty=1.2)[0]\n",
        "  decoded_output = tokenizer.decode(gen_output, skip_special_tokens=True)\n",
        "  print(decoded_output)\n",
        "\n",
        "ft_one.eval()\n",
        "with torch.no_grad():\n",
        "  gen_output = ft_one.generate(**input, max_new_tokens=100, repetition_penalty=1.2)[0]\n",
        "  decoded_output = tokenizer.decode(gen_output, skip_special_tokens=True)\n",
        "  print(decoded_output)\n",
        "\n",
        "ft_overfit.eval()\n",
        "with torch.no_grad():\n",
        "  gen_output = ft_overfit.generate(**input, max_new_tokens=100, repetition_penalty=1.2)[0]\n",
        "  decoded_output = tokenizer.decode(gen_output, skip_special_tokens=True)\n",
        "  print(decoded_output)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WkcKsg1sHAMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyfzvb7InMxo",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import GenerationConfig\n",
        "import csv\n",
        "\n",
        "# Now, let's test the pretrained model's performance vs our finetuned model's performance on our held out dataset (test_ds)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-v0.1\",\n",
        "    padding_side=\"left\",\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "total_base_loss = 0\n",
        "total_ft_one_loss = 0\n",
        "total_ft_overfit_loss = 0\n",
        "\n",
        "target_responses = []\n",
        "tokenized_inputs = []\n",
        "\n",
        "base.eval()\n",
        "ft_one.eval()\n",
        "ft_overfit.eval()\n",
        "with torch.no_grad():\n",
        "  for item in test_ds:\n",
        "    input = \"Instruction: \" + item['instruction']\n",
        "    if item['context']:\n",
        "      input += \" \\n\\n Context: \" + item['context']\n",
        "    input += \"\\n\\nResponse: \" # inputs are instruction: {instruction} context: {context} response:\n",
        "\n",
        "    target_response = item['response']\n",
        "    input_output_pair = input + target_response\n",
        "\n",
        "    tokenized_input = tokenizer.encode(input, return_tensors=\"pt\").to(\"cuda\")\n",
        "    tokenized_target = tokenizer.encode(input_output_pair, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # prep for future generation step\n",
        "    tokenized_inputs.append(tokenized_input)\n",
        "    target_responses.append(target_response)\n",
        "\n",
        "    # Compute loss. input = target, output = target\n",
        "    base_model_output = base(tokenized_target, labels=tokenized_target)\n",
        "    base_model_loss = base_model_output.loss.item()\n",
        "\n",
        "    ft_one_output = ft_one(tokenized_target, labels=tokenized_target)\n",
        "    ft_one_loss = ft_one_output.loss.item()\n",
        "\n",
        "    ft_overfit_output = ft_overfit(tokenized_target, labels=tokenized_target)\n",
        "    ft_overfit_loss = ft_overfit_output.loss.item()\n",
        "\n",
        "    total_base_loss += base_model_loss\n",
        "    total_ft_one_loss += ft_one_loss\n",
        "    total_ft_overfit_loss += ft_overfit_loss\n",
        "\n",
        "average_base_loss = total_base_loss / len(test_ds)\n",
        "average_ft_one_loss = total_ft_one_loss / len(test_ds)\n",
        "average_ft_overfit_loss = total_ft_overfit_loss / len(test_ds)\n",
        "\n",
        "base_perplexity = np.exp(average_base_loss)\n",
        "ft_perplexity = np.exp(average_ft_one_loss)\n",
        "ft_overfit_perplexity = np.exp(average_ft_overfit_loss)\n",
        "\n",
        "print(f\"Average Base Model Loss: {average_base_loss}\")\n",
        "print(f\"Average FT Loss: {average_ft_one_loss}\" )\n",
        "print(f\"Average FT Overfit Loss: {average_ft_overfit_loss}\")\n",
        "print(f\"Base Model Perplexity: {base_perplexity}\")\n",
        "print(f\"FT Perplexity: {ft_perplexity}\")\n",
        "print(f\"FT Overfit Perplexity: {ft_overfit_perplexity}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Average Base Model Loss: 2.0458536679148676\n",
        "- Average FT Loss: 1.5839086458086968\n",
        "- Average FT Overfit Loss: 1.4659164778441192\n",
        "- Base Model Perplexity: 7.735759489672113\n",
        "- FT Perplexity: 4.873969248109109\n",
        "- FT Overfit Perplexity: 4.331511155775555"
      ],
      "metadata": {
        "id": "I0oRX6Uu5azV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(tokenized_inputs, 'tokenized_inputs.pt')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZsQDE3nn5ann"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_inputs list already filled above.\n",
        "\n",
        "# Optionally calculate bleu and rouge scores!\n",
        "\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "tokenized_inputs = torch.load('tokenized_inputs.pt')\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    max_new_tokens=256,\n",
        "    num_return_sequences=1,\n",
        "    repetition_penalty=1.2\n",
        ")\n",
        "\n",
        "base_model_responses = []\n",
        "ft_one_responses = []\n",
        "ft_overfit_responses = []\n",
        "\n",
        "base.eval()\n",
        "ft_one.eval()\n",
        "ft_overfit.eval()\n",
        "with torch.no_grad():\n",
        "  for tokenized_input in tokenized_inputs:\n",
        "      # Generate responses and append to response lists\n",
        "      tokenized_input = tokenized_input.to(\"cuda\")\n",
        "      base_output = base.generate(tokenized_input, generation_config=generation_config)[0]\n",
        "      ft_one_output = ft_one.generate(tokenized_input, generation_config=generation_config)[0]\n",
        "      ft_overfit_output = ft_overfit.generate(tokenized_input, generation_config=generation_config)[0]\n",
        "\n",
        "      base_output_decoded = tokenizer.decode(base_output, skip_special_tokens=True)\n",
        "      ft_output_decoded = tokenizer.decode(ft_one_output, skip_special_tokens=True)\n",
        "      ft_overfit_output_decoded = tokenizer.decode(ft_overfit_output, skip_special_tokens=True)\n",
        "\n",
        "      base_model_responses.append(base_output_decoded)\n",
        "      ft_one_responses.append(ft_output_decoded)\n",
        "      ft_overfit_responses.append(ft_overfit_output_decoded)\n",
        "\n",
        "print(f\"Base Model Bleu Score: {bleu.compute(predictions=base_model_responses, references=[[r] for r in target_responses])['bleu']}\")\n",
        "print(f\"Base Model Rouge Score: {rouge.compute(predictions=base_model_responses, references=target_responses)['rouge1']}\")\n",
        "\n",
        "print(f\"FT One Bleu Score: {bleu.compute(predictions=ft_one_responses, references=[[r] for r in target_responses])['bleu']}\")\n",
        "print(f\"FT One Rouge Score: {rouge.compute(predictions=ft_one_responses, references=target_responses)['rouge1']}\")\n",
        "\n",
        "print(f\"FT Overfit Bleu Score: {bleu.compute(predictions=ft_overfit_responses, references=[[r] for r in target_responses])['bleu']}\")\n",
        "print(f\"FT Overfit Rouge Score: {rouge.compute(predictions=ft_overfit_responses, references=target_responses)['rouge1']}\")\n"
      ],
      "metadata": {
        "id": "ZThoz3YZMiYR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "collapsed_sections": [
        "H-090jrhIkPf",
        "nG1_SrroIpMf",
        "pp5sq8G3c271"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}