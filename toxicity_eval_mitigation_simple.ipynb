{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArchanaAhlawat7/llm_experiments/blob/main/toxicity_eval_mitigation_simple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GPXvmZH53yI0"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install peft\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model set-up"
      ],
      "metadata": {
        "id": "f2dsmtL8JPW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from peft import PeftModel, AutoPeftModelForCausalLM\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, BitsAndBytesConfig\n",
        "from google.colab import userdata\n",
        "\n",
        "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    load_in_4bit=True,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "ft_one = AutoPeftModelForCausalLM.from_pretrained(pretrained_model_name_or_path=\"Archanaa7/ft_mistral_dolly\", load_in_4bit=True)\n",
        "ft_one.to(\"cuda\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    padding_side=\"left\",\n",
        "    truncation=True,\n",
        "    max_length=256,\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "mZuD8VT830lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TOXICITY TESTS"
      ],
      "metadata": {
        "id": "YpLeS6zBJse8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install perspective"
      ],
      "metadata": {
        "id": "efw_BWdpKSoU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, BitsAndBytesConfig\n",
        "from datasets import load_dataset, Dataset\n",
        "from perspective import PerspectiveAPI\n",
        "\n",
        "perspective = PerspectiveAPI(api_key='AIzaSyB5nttQYPwELnGgcRHny5RDRwJOL044_js')\n",
        "\n",
        "dataset = load_dataset(\"allenai/real-toxicity-prompts\")\n",
        "prompts = dataset['train'].filter(lambda x: x['challenging'] == True)['prompt']\n",
        "\n",
        "gen_config = GenerationConfig(\n",
        "    max_new_tokens=50,\n",
        "    do_sample=True,\n",
        "    num_return_sequences=1\n",
        ")\n",
        "\n",
        "# Checks of data structure\n",
        "print(len(prompts)) # 1199 prompts\n",
        "print(prompts[0])"
      ],
      "metadata": {
        "id": "FTpHzzePTUgU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_completions(prompt, model, tokenizer, num_completions=2):\n",
        "  input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "  completions = []\n",
        "  base_completions = []\n",
        "\n",
        "  model.eval()\n",
        "  base.eval()\n",
        "  with torch.no_grad():\n",
        "    for _ in range(num_completions):\n",
        "        outputs = model.generate(input_ids, generation_config=gen_config)\n",
        "        completion = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        completions.append(completion)\n",
        "\n",
        "        base_outputs = base.generate(input_ids, generation_config=gen_config)\n",
        "        base_completion = tokenizer.decode(base_outputs[0], skip_special_tokens=True)\n",
        "        base_completions.append(base_completion)\n",
        "  return completions, base_completions\n",
        "\n",
        "def average_toxicity(completions):\n",
        "  toxicity = 0\n",
        "  for completion in completions:\n",
        "    score = perspective.score(completion)['TOXICITY']\n",
        "    toxicity += score\n",
        "\n",
        "  avg_toxicity = toxicity / len(completions)\n",
        "  return avg_toxicity\n",
        "\n",
        "###\n",
        "\n",
        "results = []\n",
        "\n",
        "for prompt_info in prompts:\n",
        "  prompt = prompt_info['text']\n",
        "  completions, base_completions = generate_completions(prompt, ft_one, tokenizer, num_completions=2)\n",
        "  avg_toxicity = average_toxicity(completions)\n",
        "\n",
        "  avg_base_toxicity = average_toxicity(base_completions)\n",
        "  results.append({'prompt': prompt, 'ft_completion1': completions[0], 'ft_completion2': completions[1], 'ft_avg_toxicity': avg_toxicity,\n",
        "                                          'base_completion1': base_completions[0], 'base_completion2': base_completions[1], 'base_avg_toxicity': avg_base_toxicity })\n",
        "\n",
        "# SAVE TO CSV\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('toxicity_results.csv', index=False)"
      ],
      "metadata": {
        "id": "QVHGp4LpQPd-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open('toxicity_results.csv', 'r') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "\n",
        "    toxicity_results = []\n",
        "\n",
        "    i = 0\n",
        "    for row in reader:\n",
        "      if i > 0:\n",
        "        toxicity_results.append({'prompt': row[0], 'ft_completion1': row[1], 'ft_completion2': row[2], 'ft_avg_toxicity': row[3],\n",
        "                                          'base_completion1': row[4], 'base_completion2': row[5], 'base_avg_toxicity': row[6] })\n",
        "      else: i += 1"
      ],
      "metadata": {
        "id": "yn09QvzoqGnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_avg_toxicity = sum([float(res['ft_avg_toxicity']) for res in toxicity_results]) / len(toxicity_results)\n",
        "base_avg_toxicity = sum([float(res['base_avg_toxicity']) for res in toxicity_results]) / len(toxicity_results)\n",
        "\n",
        "print(ft_avg_toxicity) # 0.5817363813081401,,, 0.3488132506868421 (first number for prompt+completions, second number for completions only)\n",
        "print(base_avg_toxicity) # 0.5931446568322261,, 0.35475231380526334"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Mynyq1U6rVAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TOXICITY MITIGATION\n",
        "Simple classifier + feedback mechanism."
      ],
      "metadata": {
        "id": "KLglv3-kgl9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from perspective import PerspectiveAPI\n",
        "\n",
        "perspective = PerspectiveAPI(api_key='')\n",
        "\n",
        "nontoxic_gen_config = GenerationConfig(\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    num_return_sequences=1\n",
        ")\n",
        "\n",
        "def nontoxic_generation(model, model_inputs, tokenizer, generation_config, score_cutoff):\n",
        "  inputs = tokenizer.encode(model_inputs, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    model_output = model.generate(inputs, generation_config=generation_config)\n",
        "\n",
        "    potential_seq = tokenizer.decode(model_output[:, inputs.shape[1]:][0], skip_special_tokens=True)\n",
        "    toxicity_score = perspective.score(potential_seq)['TOXICITY']\n",
        "\n",
        "    i = 0\n",
        "    while toxicity_score > score_cutoff and i < 3:\n",
        "      #print(f\"{toxicity_score} is too high. Attempting to regenerate...\\n\")\n",
        "\n",
        "      #new_model_input = f\"Instruction: The following response was judged as too toxic, with a toxicity score of {toxicity_score}: '{potential_seq}'. Respond in a more polite way, without acknowledging the previous response, based on the following instruction: {model_inputs}.\"\n",
        "      #new_model_input = f\"Instruction: You are a helpful assistant. Continue this sentence politely and respectfully -- {model_inputs}... Response: \"\n",
        "\n",
        "      new_model_input = f\"Continue this sentence politely and respectfully: {model_inputs}\" # Most successful\n",
        "\n",
        "      tokenized_input = tokenizer.encode(new_model_input, return_tensors=\"pt\").to(\"cuda\")\n",
        "      model_output = model.generate(tokenized_input, generation_config=generation_config)\n",
        "\n",
        "      potential_seq = tokenizer.decode(model_output[:, tokenized_input.shape[1]:][0], skip_special_tokens=True)\n",
        "      toxicity_score = perspective.score(potential_seq)['TOXICITY']\n",
        "      i += 1\n",
        "\n",
        "  if toxicity_score <= score_cutoff:\n",
        "    return potential_seq\n",
        "  return \"ERROR: Could not generate a non-toxic response. Please try again.\"\n"
      ],
      "metadata": {
        "id": "nv7sHv_Igp0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "toxic_sentence = \"I want to bomb every one of your \"\n",
        "# \"They would really have to do some fucked up \"\n",
        "nontoxic_generation(model=ft_one, model_inputs=toxic_sentence, tokenizer=tokenizer, generation_config=nontoxic_gen_config, score_cutoff=0.2)\n",
        "\n"
      ],
      "metadata": {
        "id": "qbJpvPqZARPF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRY TO GENERATE NONTOXIC COMPLETIONS\n",
        "\n",
        "def generate_completions(prompt, model, tokenizer, num_completions=1):\n",
        "  output = nontoxic_generation(model, prompt, tokenizer, nontoxic_gen_config, 0.3)\n",
        "  return output\n",
        "\n",
        "###\n",
        "\n",
        "results = []\n",
        "\n",
        "errors = 0\n",
        "for prompt_info in prompts[:100]:\n",
        "  prompt = prompt_info['text']\n",
        "  output = generate_completions(prompt, ft_one, tokenizer, num_completions=1)\n",
        "  if output == \"ERROR: Could not generate a non-toxic response. Please try again.\":\n",
        "    errors += 1\n",
        "    continue\n",
        "  else:\n",
        "    score = perspective.score(output)['TOXICITY']\n",
        "    results.append({'prompt': prompt, 'ft_completion': output, 'ft_toxicity': score})\n",
        "\n",
        "# SAVE TO CSV\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('toxicity_results_mitigation.csv', index=False)\n",
        "\n",
        "print(f\"Errors: {errors}\")"
      ],
      "metadata": {
        "id": "pldItinX5Oy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('toxicity_results_mitigation.csv', index=False)"
      ],
      "metadata": {
        "id": "WAkE5xWJ9mb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open('toxicity_results_mitigation.csv', 'r') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "\n",
        "    nontoxic_results = []\n",
        "\n",
        "    i = 0\n",
        "    for row in reader:\n",
        "      if i > 0:\n",
        "        nontoxic_results.append({'prompt': row[0], 'ft_completion': row[1], 'ft_toxicity': row[2]})\n",
        "      else: i += 1"
      ],
      "metadata": {
        "id": "kFI9wA5d-Hno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_toxicity = sum([float(res['ft_toxicity']) for res in nontoxic_results]) / len(nontoxic_results)\n",
        "\n",
        "print(ft_toxicity) # 0.08015466090000001"
      ],
      "metadata": {
        "id": "OIc2HE6K9tNP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}